\chapter{Conclusions}
% ...

The approach we presented allows us to obtain acceptable performance, but it is strongly linked to the quality 
and selection of the datasets of both models. In particular it is important to use larger Language Models.

Our experimentation with different datasets has made us come to the conclusion that, for the models to work 
optimally, each dataset must share the same language. That is, the datasets must share most of the vocabulary 
used in them. We have noticed that when the datasets don't share the same vocabulary, and the distribution of 
words isn't aligned between them, the error model might produce candidates that it found most probable for 
the language model he's trained for, but in the HMM, if trained on a transitions dataset not sharing the same 
language as the language model, might discard these candidates for less-likely ones that have a higher 
transition probability.


In the future, we can generalised some assumptions and made some improvements, 


- update the various dataset: in particular we can use a new frequency dataset consistent with the language model one.\\
- Compute the initial probabilities $\pi$ from the language model\\
- Consider additional type of error in the model\\
- improving the noisy channel error model to consider neighbouring characters
- Use other algorithms, such as the forward-backward (smooothing) instead Viterbi\\
- Optimise the code, in particular the lemmatisation/stemming \\
- Use recurrent neural network instead the hidden markov models.\\
- We can also improve the performance of the noisy channel model by changing how the prior and the likelihood 
are combined. In the standard model they are just multiplied together.

The fact that words are generally more frequent than their misspellings can be used in candidate suggestion, by 
building a set of words and spelling variations that have similar contexts, sorting by frequency, treating the most 
frequent variant as the source, and learning an error model from the difference