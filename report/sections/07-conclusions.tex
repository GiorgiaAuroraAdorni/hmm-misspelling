\chapter{Conclusions}
% ...

it is important to use larger language models than unigrams


For this reason modern systems often use much larger dictionaries automatically derived from very large lists of 
unigrams like the Google N-gram corpus

\section{Future Works}

- update the various dataset: in particular we can use a new frequency dataset consistent with the language model one.\\
- Compute the initial probabilities $\pi$ from the language model\\
- Consider additional type of error in the model\\
- improving the noisy channel error model to consider neighbouring characters
- Use other algorithms, such as the forward-backward (smooothing) instead Viterbi\\
- Optimise the code, in particular the lemmatisation/stemming \\
- Use recurrent neural network instead the hidden markov models.\\
- We can also improve the performance of the noisy channel model by changing how the prior and the likelihood 
are combined. In the standard model they are just multiplied together.

The fact that words are generally more frequent than their misspellings can be used in candidate suggestion, by 
building a set of words and spelling variations that have similar contexts, sorting by frequency, treating the most 
frequent variant as the source, and learning an error model from the difference