\chapter{Dataset}

%creazione del training set .. 

%Per ogni modello, la creazione dei set di training e validation è stata 
%realizzata utilizzando due tecniche diverse: in un primo esperimento è 
%stato 
%utilizzato un \textit{holdout} 80--20, in un secondo si è ricorso ad una 
%\textit{10-fold cross validation}.

\section{Dataset acquisition}
\label{section:dataset-exploration}
\subsection{Error Model}
This dataset was collected from the following resources 
\footnote{\url{https://www.dcs.bbk.ac.uk/~ROGER/corpora.html}}  
\footnote{\url{https://www.kaggle.com/rtatman/spelling-variation-on-urban-dictionary}}  
\footnote{\url{https://www.kaggle.com/bittlingmayer/spelling}}
\footnote{\url{http://luululu.com/tweet}}:
\begin{itemize}
	\item \textsc{birkbeck}: contains \num{36133} misspellings of \num{6136} words, taken from the native-speaker 
	section (British and American) of the Birkbeck spelling error corpus.
	\item \textsc{holbrook}: contains \num{1791} misspellings of \num{1200} words, taken from the book "English for the 
	Rejected" by 
	David Holbrook (Cambridge University Press - 1964).
	\item \textsc{aspell}: contains \num{531} misspellings of \num{450} words, taken from one assembled by Atkinson for 
	testing the 
	GNU Aspell spellchecker.
	\item \textsc{wikipedia}: contains \num{2455} misspellings of \num{1922} words, taken from the misspellings made by 
	Wikipedia 
	editors.
	\item \textsc{urban-dictionary-variants}: contains \num{716} variant spellings, taken from the text scraped from Urban 
	Dictionary (in UK English).
	\item \textsc{spell-set}: contains \num{670} typos.
	\item \textsc{tweet-typo}: contains \num{39172} typos, taken from Twitter.
\end{itemize}

All the datasets are joined and cleaned. After that, it contains \num{39888} row, in each of which there 
is the typo and the correct word.
The dataset obtained is divided into two corpora: \num{80}\% is used as a train set (\num{31808} row) and \num{20}\% is 
used as a test set (\num{8080} row).

\subsection{Language Model}
\label{subsection:languagemodel}
The language model dataset is a concatenation of public domain book excerpts from 
\href{http://www.gutenberg.org/wiki/Main_Page}{ \textcolor{blue}{Project Gutenberg}}  and lists of most frequent words 
from \href{https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists}{ \textcolor{blue}{Wiktionary}} and the 
\href{http://www.kilgarriff.co.uk/bnc-readme.html}{\textcolor{blue}{British National Corpus}}. 

This dataset contains about a million word. It is use to estimate the probability of a word by counting the number of times 
each word appears in this dataset. %Fixme: spiegare per cosa usiamo questo dataset  To do that, the text was breaks into 
%words, then a variable holds a counter of how often each word appears. Than the  probability of each word, based on 
%this Counter was estimated

\subsection{Frequency Model}
The words frequency dataset includes wordlists derived from the \href{https://books.google.com/ngrams/}{ 
\textcolor{blue}{Google's ngram corpora}}. In particular we use the dataset \texttt{frequency-alpha-gcide.txt}, a smaller 
version of the original dataset, cleaned up and limited to only the top \num{65538} words.

Each row of the corpus contains the ranking of the word, the word itself, the count of the occurrences of the word, a 
percentage of how often each word was being used and a cumulative percentage.

With the following dataset we found some problems due to the lack of proper names, city names, countries, brands etc.

\subsection{Perturbated Dataset}
In order to evaluate our algorithm on whole sentences, we create a new perturbed dataset starting from the dataset 
\texttt{big} described in the section \ref{subsection:languagemodel}.

The disturbance introduced presents an error partly dependent on the error model previously presented, with the 
difference that it is created starting from the typos belonging to the test dataset.

Three different texts have been generated, each of which has a percentage of errors in the text of \num{10}-\num{15}-
\num{20}\% respectively. \\

We implemented a perturbation algorithm, which for each line of our input file generates a new perturbed string.\\
The input text is perturbed accordingly the following steps:

\begin{enumerate}
	\item Give us from the error model the value of $p$, the probability that a word has an edit, for each word  of length 
	$n$, the number of edits to be introduced $x$ is calculated according to the relation $ x \sim \text{Bin}(n, 
	p)$
	\item FIXME % How are the character positions to be changed chosen?
	\item FIXME % How is the type of edit chosen?
	\item The disturbance goes to alter the letters designated not in a random manner. 
	In fact, we use three different probabilities to define whether a letter will be deleted from the index in question, if a new 
	letter will be hung (FIXME: come), or if the current character will be replaced with one of the possible letters (coming 
	from the error model).
	
\end{enumerate}

Cases of elimination of a whole word are excluded, as these would heavily influence the evaluation metrics as they are 
inconsistent with our model.

%(\num{58000} sentence)

