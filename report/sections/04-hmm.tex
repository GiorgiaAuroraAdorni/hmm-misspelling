We will consider two optimality criteria. The first one chooses the states that are individually most likely and maximizes the 
expected number of correct individual states. The second criterion estimates the most likely state sequence, or 
\textit{trellis path}. The algorithm used to implement these criteria is the \textbf{Viterbi} algorithm. %\ref

\section{Noisy Channel Model}

\textbf{FIXME}: Real word spelling error detection is a much more difficult task, since any word in the input text could be 
an error. So we don’t...\\


Estimates for the frequency of spelling errors in human-typed text vary from 1-2\% for carefully retyping already printed 
text to 10-15\% for web queries.

Non-word errors are detected by looking for any word not found in a dictionary. To correct non-word spelling errors we 
first generate candidates: real words that have a similar letter sequence to the error. FIXME: We generates candidates 
according to distance given as a parameter to te model (max\_edit\_distance). 

The intuition of the noisy channel model (see Fig. B.1) is to treat the misspelled word as if a correctly spelled word had 
been “distorted” by being passed through a noisy communication channel. 
This channel introduces “noise” in the form of substitutions or other changes to the letters, making it hard to recognise 
the “true” word. 

This noisy channel model is a kind of Bayesian inference. We see an observation x (a misspelled word) and our job is to 
find the word w that generated this misspelled word.
Out of all possible words in the vocabulary V we want to find the word w such that P(w|x) is highest. We use the hat 
notation ˆ to mean “our estimate of the correct word”.
The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. B.1 into a set of other probabilities.
But P(x) doesn’t change for each word; we P(x)
are always asking about the most likely word for the same observed error x, which must have the same probability P(x).
The prior probability of a hidden word is modelled by P(w). 

We apply the noisy channel approach to correcting non-word spelling errors by taking any word not in our spell 
dictionary, generating a list of candidate words, ranking them according to Eq. B.4, and picking the highest-ranked one.

\[\overbrace{w} = \arg\max_{w \in V} P(w|x) = \arg\max_{w \in V} \frac{P(x|w)P(w)}{P(x)} = \arg\max_{w \in V} 
{P(x|w)P(w)}\]


This model is a sort of a local corrector. It consists of two components: a source model, corresponding to 
$P(\text{candidate})$, and a channel model, 
that is $P(\text{typo}|\text{candidate})$.


Given an input typo, for example \textsl{adventhre}, the first stage generates a set of candidate corrections for the word, 
for example \textsl{adventure}, \textsl{adventurer}, \textsl{adventured}…
Each candidate is scored by $P(\text{candidate})P(\text{typo}|\text{candidate})$ and then normalised by the sum of the 
scores for all proposed candidates.

\textbf{FIXME}:  (How is estimated the prior? How are computed the conditional probabilities?)

We choose the most likely candidate, the one with the highest probability.

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{NoisyChannel.png}
	\caption{Noisy channel}
	\label{fig:noisychannel}
\end{figure}

\textbf{FIXME}: When for a input typo we do not have any candidate ?? 


\subsection{Common Errors}
The following type of errors were considered:
\begin{itemize}
	\item \textsc{transposed adjacent characters}
	\item \textsc{omitted digit}
	\item \textsc{additional digit}
	\item \textsc{substituted digit}: this is the probability to type a
	character $i$ when the character $j$ was intended ($P(i|j)$). This 
	probability was determined experimentally. \textbf{FIXME}: how? from where?
\end{itemize}

\subsection{Edit Distance Algorithm}
To find this list of candidates we’ll use the minimum edit distance algorithm introduced in Chapter 2, but extended so that 
in addition to insertions, deletions, and substitutions, we’ll add a fourth type of edit, transpositions, in which two letters 
are swapped.
The version of edit distance with transposition is called Damerau-Levenshtein edit distance


\section{Most Likely State Sequence}

The Viterbi algorithm calculates the most probable sequence of hidden states, the words intended.

The initial probability of being in a state $i$, $\pi_i$, in our case the probability of intend a word $i$, and the 
transition probabilities $A_{ij}$, or the transition from the word $i$ to the next word $j$, are given. Since we have 
observed the output $y_1, y_2, \dots , y_t$, that is the sentence written with typos, it is possible to computed the most 
likely state sequence $x_1, x_2, \dots , x_t$, the sentence intended, starting from the following expression:

\begin{equation}
	\begin{aligned}
		V_{1,t+1} &= P(x_1, \dots, x_t, x_{t+1}, y_1, \dots, y_t,  y_{t+1}) = \\
						&= \arg\max_{x_{1:t}} p(x_1, \dots, x_t | y_1, \dots, y_t) = \\
						& =  \alpha \cdot p(y_{t+1}|x_{t+1})\cdot\max_{x_t} \Big( p(x_{t+1}|x_t) \max p(x_1, \dots, x_{t}|y_1, 
						\dots, y_t)\Big)
	\end{aligned}
\end{equation}

%FIXME
The initial state probabilities $\pi$ are actually the word frequencies (? We don’t estimate it in a proper way), the state 
transition probabilities are given by the probability of a word given its predecessor, and the emission probabilities are the 
probabilities to type word $i$ when word $j$ was intended.

In our implementation, we construct the \textit{trellis} choosing the locally/globally best state. As we can see in the 
picture below, we display an example of the trellis drawing only the best predecessor of each state.

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{TrellisExample.png}
	\caption{Trellis example}
	\label{fig:trellis}
\end{figure}

In this example, we observed the sentence \textsl{my practice iu never iery absorcing} and our algorithm return the most 
likely state sequence \textsl{may practice is never very absorbing}. In this case, the algorithm partially fails, because the 
intended sentence was \textsl{my practice is never very absorbing}.
\\
\textbf{FIXME}: (pi la probabilità iniziale del prima stato non lo abbiamo, non lo facciamo perchè dal nostro dataset non 
abbiamo sempre la prima parola…)

We decide to implement the \textbf{Viterbi} based algorithm instead the Forward-Backward algorithm relying on the 
experiments carried out in the literature.
The HMM-Based Error Correction Mechanism for Five-Key Chording Keyboards article \cite{tarniceriu2015hmm} explains 
that the Forward-Backward algorithm estimates the most likely state for each observation, but the resulting state 
sequence may not be a valid succession of words in natural language (or a very unlikely word sequence) and produce 
inferior results.


\section{Hidden Markov Model}


\textbf{FIXME}: Hmm approach
\textbf{FIXME}: description of hmm

\textbf{FIXME}: our application ...