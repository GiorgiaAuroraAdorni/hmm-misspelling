\textbf{FIXME} comparison
\textbf{FIXME} performance

\subsection{Evaluation Metrics}
We evaluate the local model performances, or that carried out on individual typos, through various measures of 
accuracy. 
We compute the \textsc{Top-1 Accuracy} comparing the misspelt word and the best candidate predicted by the 
model. 
Than we also compute the \textsc{Top-3 Accuracy} and \textsc{Top-5 Accuracy}, comparing the misspelt word 
with the first $N$ candidates, in this case \num{3} and \num{5}, given from the model.\\


As regards the performance evaluation of the entire sequences, after every training and prediction we save a csv 
file containing the sentences with the perturbations, the sentences that we want to predict (hidden truth) and the 
sentences provided by the model.

The idea to evaluate the performance of the model is very simple: we scroll through the lines containing the 
perturbed, the predicted and the original (intended) text and, for each word we verify if it was disturbed or not and 
if it corresponds to the original truth.
Therefore, for each word, the following cases can occur:
\begin{enumerate}
	\item \textit{Perturbed word not correctly provided}
	\begin{enumerate}
		\item the word was not the subject of attempted correction by the model
		\item the word has been the subject of attempted correction by the model but without success
	\end{enumerate}
	\item \textit{Perturbed word correctly provided}
	\item \textit{Unperturbed word not correctly provided}
	\item \textit{Unperturbed word correctly provided}
\end{enumerate}

We can therefore represent the confusion matrix of the data in output through table \ref{tab:confmat}:

\begin{figure}[H]
	\centering
	\begin{tabular}{lc|cc}
		\toprule
		& & \multicolumn{2}{c}{\textbf{Model prediction}}\\
		& & \textsc{True}  & \textsc{False} \\
		\midrule
		\multirow{4}{*}{\textbf{Hidden Truth}} 
		& \multirow{2}{*}{\textsc{True}}   & True Positive & False Negative	\\ 
			& & Case 2. & Case 1(a).	\\ 
		& \multirow{2}{*}{\textsc{False}}  & False Positive & True Negative	\\
		& &  Case 3. \& 1(b).  & Case 4.	\\ 
		\bottomrule
	\end{tabular}
	\captionof{table}{Confusion matrix}
	\label{tab:confmat}
\end{figure}

From the confusion matrix defined in the table above, it is possible to calculate the following performance 
metrics in a simple way:
\begin{itemize}
	\item \textsc{Rate of perturbed words correctly predicted}:
	\[ \frac{\mbox{True Positive}}{\sum \mbox{Perturbed}} = \frac{\mbox{Case 2.}}{\mbox{Case 2.} + \mbox{Case 
	1.}}\]
	\item \textsc{Rate of unperturbed words not correctly predicted}:
	\[ \frac{\mbox{True Negative}}{\sum \mbox{Unperturbed}} = \frac{\mbox{Case 2.}}{\mbox{Case 3.} + 
	\mbox{Case 4.}}\]
	\item \textsc{Accuracy}: measures the goodness of the model among the positive correction results obtained
	\[ \frac{\mbox{True Positive} + \mbox{True Negative}}{\sum \mbox{All}} = \frac{\mbox{Case 2.} + \mbox{Case 
	4.}}{\mbox{Case 1.} + \mbox{Case 2.} + \mbox{Case 3.} + \mbox{Case 4.}}\]
	\item \textsc{Precision}: proportion of all the correct results \textbf{FIXME}
		\[ \frac{\mbox{True Positive}}{\mbox{True Positive} + \mbox{False Positive}} = \frac{\mbox{Case 
		2.}}{\mbox{Case 1(b).} + \mbox{Case 2.} + \mbox{Case 3.}}\]
	\item \textsc{Recall}: ration of the corrected predictions with respect to the significant values returned by the 
	model (the corrections made) \textbf{FIXME}
		\[ \frac{\mbox{True Positive}}{\mbox{True Positive} + \mbox{False Negative}} = \frac{\mbox{Case 
		2.}}{\mbox{Case 1(a).} + \mbox{Case 2.}}\]
	\item \textsc{F-Measure}: weighted average (between 0 and 1) with respect to precision and recall
			\[ 2 * \frac{\mbox{Precision} * \mbox{Recall}}{\mbox{Precision} + \mbox{Recall}} \]
\end{itemize}

Based on these metrics, it is then possible to define whether the models have behaved more or less correctly.

\subsection{Experiments}
We performed three different types of experiments.

The first one using as a transition model the dataset \texttt{big\_clean}, the associate perturbed dataset and test error 
models, and the language model \texttt{frequency-alpha-gcide}.

A second one using the same datasets but introducing a lemmatisation consisting in a simple dictionary lookup.

The last one using as transition model the dataset \texttt{lotr\_clean}, the associate perturbed dataset and test error 
models, and the language model \texttt{lotr\_language\_model}.\\

In all the experiments to come, reference will be made to the following variables:
\begin{itemize}
	\item p: the probability that a word has an edit
	\item ins: the probability that a word has a letter insertion
	\item del: the probability that a word has a letter deletion
	\item sub: the probability that a word has a letter substitution
	\item swap: the probability that a word has a swap between two letters
\end{itemize}

\subsubsection{Experiment 1}

\textbf{FIXME}
Edit distance, perturbation dataset used


\begin{figure}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		p 				 & ins 				 	& del  				&  sub 			   &   swap\\ \midrule
		\num{0.5} & \num{0,70} & \num{0,70}  & \num{0,70} & \num{0,70}\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Error Model}
	\label{tab:error_model1}
\end{figure}

% how many observation
\begin{figure}[H]
	\centering
	\begin{tabular}{lcccc}
		\toprule
		& Time (sec)  & Accuracy Top1 & Accuracy Top3  &  Accuracy Top5 \\
		\midrule
		Train & \num{} & \%  &  \% &  \%  \\
		Test &	\num{1220}  & \num{40,96}\%  & \num{56,34} \% & \num{60,03} \%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Typos performance evaluation}
	\label{tab:typo-eval1}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{tabular}{ccccccc}
		\toprule
		\#sentence & Time (sec)  & Accuracy & Initial Error  &  Precision & Recall & Specificity \\
		\midrule
		\num{5000}	& \num{3549}  & \num{44,40}\%  & \num{14,77}\% & \num{89,43}\% & \num{45,42}\%  & 
		\num{14,81}\%  
		\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Sentences performance evaluation}
	\label{tab:sentence-eval1}
\end{figure}

\subsubsection{Experiment 2}
\begin{figure}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		p 				 & ins 				 	& del  				&  sub 			   &   swap\\ \midrule
		\num{0.5} & \num{0,70} & \num{0,70}  & \num{0,70} & \num{0,70}\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Error Model}
	\label{tab:error_model2}
\end{figure}

We detected some problems with our dataset, in particular it lacks of plural forms and other things

In order to avoid this problem, we decided to try a new approach that use lemmaisation (stemmisationj ......

\begin{figure}[H]
	\centering
	\begin{tabular}{lcccc}
		\toprule
		& Time (sec)  & Accuracy Top1 & Accuracy Top3  &  Accuracy Top5 \\
		\midrule
		Train & \num{20986} & \num{41,38}\%  & \num{57,28} \% & \num{61,60} \% \\
		Test &	\num{5270}  & \num{41,54}\%  & \num{57,18} \% & \num{61,15} \%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Typos performance evaluation}
	\label{tab:typo-eval2}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{tabular}{ccccccc}
		\toprule
		\#sentence & Time (sec)  & Accuracy & Initial Error  &  Precision & Recall & Specificity \\
		\midrule
		\num{1000}	& \num{2705}  & \num{53,46}\%  & \num{15,01}\% & \num{90,96}\% & \num{54,50}\%  & 
		\num{10,52}\%  
		\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Sentences performance evaluation}
	\label{tab:sentence-eval2}
\end{figure}

\subsubsection{Experiment 3}

Edit distance, perturbation dataset used

\begin{figure}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		p 				 & ins 				 	& del  				&  sub 			   &   swap\\ \midrule
		\num{0.5} & \num{0,70} & \num{0,70}  & \num{0,70} & \num{0,70}\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Error Model}
	\label{tab:error_model3}
\end{figure}

Analysis of spelling error data has shown that the majority of spelling errors consist of a single-letter change and 
so we often make the simplifying assumption that these candidates have an edit distance of 1 from the error 
model.

\textbf{FIXME}
\begin{figure}[H]
	\centering
	\begin{tabular}{lcccc}
		\toprule
		& Time (sec)  & Accuracy Top1 & Accuracy Top3  &  Accuracy Top5 \\
		\midrule
		Train & \num{20986} & \num{41,38}\%  & \num{57,28} \% & \num{61,60} \% \\
		Test &	\num{5270}  & \num{56,78}\%  & \num{74,75} \% & \num{80,59} \%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Typos performance evaluation}
	\label{tab:typo-eval3}
\end{figure}

% con 10% ho il 17% di errore
% con 10% ho il 25% di errore
% con 20% ho il 40% 31 di errore

\begin{figure}[H]
	\centering
	\begin{tabular}{lccc|ccc}
		\toprule
		\textbf{Edit Distance} & \multicolumn{3}{c|}{1} & \multicolumn{3}{c}{2}\\
		\textbf{Dataset Perturbation} & \num{10}\% & \num{15}\%& \num{20}\% & \num{10}\% & \num{15}\%& 
		\num{20}\% \\
		\midrule
		Time (sec) &\num{165}&\num{157}& \num{147}&\num{2920}&\num{2835}&\num{2869}\\
		\midrule
		Perturbed correct & \num{81,33}\% &\num{76,48}\%& \num{71,58}\%& \num{61,33}\% 
		&\num{60,99}\% 
		&\num{59,91}\%\\
		Unperturbed not correc. &\num{41,62}\%&\num{42,34}\% & \num{43,94}\% & \num{61,75}\% & 
		\num{61,61}\% & 
		\num{62,09}\%\\
		Exact match &\num{5,49}\%&\num{5,35}\%&\num{5,06}\%&\num{1,76}\%&\num{2,06}\%&\num{2,06}\%\\
		Accuracy &\num{60,84}\% &\num{61,49}\% &\num{60,59}\% &\num{40,70}\% &\num{43,08}\% 
		&\num{44,39}\% \\
		Precision&\num{21,94}\%&\num{33,61}\% &\num{39,7}\%&\num{12,41}\%&\num{21,22}\%&\num{27,06}\%\\
		Recall&\num{94,59}\%&\num{90,05}\%&\num{85,97}\%&\num{99,41}\%&\num{98,58}\%&\num{97,71}\%\\
		F-Measure&\num{50,55}\%&\num{54,58}\%&\num{57,09}\%&\num{39,16}\%&\num{43,67}\%&\num{47,36}\%\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Sentences performance evaluation}
	\label{tab:sentence-eval3a}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{tabular}{ccccccc}
		\toprule
		\#sentence & Time (sec)  & Accuracy & Initial Error  &  Precision & Recall & Specificity \\
		\midrule
		\num{1620}	& \num{2705}  & \num{58,21}\%  & \num{17,39}\% & \num{90,30}\% & \num{58,62}\%  & 
		\num{8,43}\%  
		\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Sentences performance evaluation}
	\label{tab:sentence-eval3}
\end{figure}