\chapter{Experiment and Results}

\section{Evaluation Metrics}
We evaluate the local model performances, or that carried out on individual typos, through various measures of 
accuracy. 
We compute the \textsc{Top-1 Accuracy} comparing the misspelt word and the best candidate predicted by the 
model. 
Than we also compute the \textsc{Top-3 Accuracy} and \textsc{Top-5 Accuracy}, comparing the misspelt word 
with the first $N$ candidates, in this case \num{3} and \num{5}, given from the model.\\


As regards the performance evaluation of the entire sequences, after every training and prediction we save a csv 
file containing the sentences with the perturbations, the sentences that we want to predict (hidden truth) and the 
sentences provided by the model.

The idea to evaluate the performance of the model is very simple: we scroll through the lines containing the 
perturbed, the predicted and the original (intended) text and, for each word we verify if it was disturbed or not and 
if it corresponds to the original truth.
Therefore, for each word, the following cases can occur:
\begin{enumerate}
	\item \textit{Perturbed word not correctly provided}
	\begin{enumerate}
		\item the word was not the subject of attempted correction by the model
		\item the word has been the subject of attempted correction by the model but without success
	\end{enumerate}
	\item \textit{Perturbed word correctly provided}
	\item \textit{Unperturbed word not correctly provided}
	\item \textit{Unperturbed word correctly provided}
\end{enumerate}

We can therefore represent two different confusion matrices, the first one based on the error detection and the 
second one on the error correction.

\begin{figure}[H]
	\centering
	\begin{tabular}{lc|cc}
		\toprule
		& & \multicolumn{2}{c}{\textbf{Model Prediction}}\\
		& & \textsc{Detected}  & \textsc{Not Detected} \\
		\midrule
		\multirow{4}{*}{\textbf{Hidden Truth}} 
		& \multirow{2}{*}{\textsc{Perturbed}}   & True Positive & False Negative	\\ 
		& &  Case 1(b). \& Case 2. & Case 1(a).	\\ 
		& \multirow{2}{*}{\textsc{Unperturbed}}  & False Positive & True Negative	\\
		& &  Case 3.  & Case 4.	\\ 
		\bottomrule
	\end{tabular}
	\captionof{table}{Confusion matrix - Error Detection}
	\label{tab:confmat-detection}
\end{figure}

From the confusion matrix defined in the table above, it is possible to calculate the following performance 
metrics in a simple way:
\begin{itemize}
	\item \textsc{Detection-Accuracy}: percentage of words where the model prediction matches the 
	ground-truth  
	\[ \frac{\mbox{True Positive} + \mbox{True Negative}}{\sum \mbox{All}} = \frac{\mbox{Case 1(b).} + 
	\mbox{Case 2.} + \mbox{Case 4.}}{\mbox{Case 1.} + \mbox{Case 2.} + \mbox{Case 3.} + \mbox{Case 4.}}\]
	\item \textsc{Detection-Recall}: proportion of all the detected errors among all the errors
	\[ \frac{\mbox{Case 1(b).} + \mbox{Case 2.}}{\mbox{Case 1.} + \mbox{Case 2.}}\]
	\item \textsc{Detection-Precision}: ratio of the corrected detection with respect to all the detections
		\[ \frac{\mbox{True Positive}}{\mbox{True Positive} + \mbox{False Positive}} = \frac{\mbox{Case 1(b). + 
		\mbox{Case 2.}}}{\mbox{Case 1(b).}+  \mbox{Case 2.} + \mbox{Case 3.}}\]
\end{itemize}


\begin{figure}[H]
	\centering
	\begin{tabular}{lc|cc}
		\toprule
		& & \multicolumn{2}{c}{\textbf{Model Prediction}}\\
		& & \textsc{Corrected}  & \textsc{Not Corrected} \\
		\midrule
		\multirow{4}{*}{\textbf{Hidden Truth}} 
		& \multirow{2}{*}{\textsc{Perturbed}}   & True Positive & False Negative	\\ 
			& & Case 2. & Case 1.	\\ 
		& \multirow{2}{*}{\textsc{Unperturbed}}  & False Positive & True Negative	\\
		& &  Case 3.  & Case 4.	\\ 
		\bottomrule
	\end{tabular}
	\captionof{table}{Confusion matrix - Error Correction}
	\label{tab:confma-error}
\end{figure}


From the second confusion matrix defined in the Table \ref{tab:confma-error}, it is possible to calculate the 
following metrics:
\begin{itemize}
	\item \textsc{Correction-Accuracy}: percentage of words where the model prediction matches the 
	ground-truth  
	\[ \frac{\mbox{True Positive} + \mbox{True Negative}}{\sum \mbox{All}} = \frac{\mbox{Case 2.} + \mbox{Case 
	4.}}{\mbox{Case 1.} + \mbox{Case 2.} + \mbox{Case 3.} + \mbox{Case 4.}}\]
	\item \textsc{Correction-Recall}: rate of perturbed words correctly predicted
\[ \frac{\mbox{True Positive}}{\mbox{True Positive} + \mbox{False Negative}} = \frac{\mbox{Case 
		2.}}{\mbox{Case 2.} + \mbox{Case 1.}}\]
	\item \textsc{Correction-Precision}: ratio of the corrected predictions with respect to the significant values 
	returned by the 
	model (the corrections made) 
		\[ \frac{\mbox{True Positive}}{\mbox{True Positive} + \mbox{False Positive}} = \frac{\mbox{Case 
		2.}}{\mbox{Case 2.} + \mbox{Case 3.}}\]
\end{itemize}

The \textsc{Specificity}, that is the rate of unperturbed words correctly predicted, is computed in the same way 
from both the confusion matrices:
\[ \frac{\mbox{True Negative}}{\sum \mbox{Unperturbed}} = \frac{\mbox{Case 4.}}{\mbox{Case 3.} + 
	\mbox{Case 4.}}\]


Based on these metrics, it is then possible to define whether the models have behaved more or less correctly.

\section{Experiments}
We performed three different types of experiments.

The first one using as a transition model the dataset \texttt{big\_clean}, the associate perturbed dataset and test error 
models, and the language model \texttt{frequency-alpha-gcide}.

A second one using the same datasets but introducing a lemmatisation consisting in a simple dictionary lookup.

The last one using as transition model the dataset \texttt{lotr\_clean}, the associate perturbed dataset and test error 
models, and the language model \texttt{lotr\_language\_model}.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment 1}
The first experiment was carried out on two different HMM that differ in the choice of the \texttt{max\_edits} 
parameter.
In the first case, in fact, the edit distance considered was $1$, while in the second $2$.
In both cases the HMM is structured as follows:

\begin{figure}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
				max states 	& language model	&  sentence ds  &  train typo ds 	&  test typo ds\\ \midrule
				\num{5} & \texttt{big\_language\_model} & \texttt{big\_clean}  & \texttt{big\_train}  &\texttt{big\_test}\\
		\bottomrule
	\end{tabular}
	\captionof{table}{HMM model}
	\label{tab:error_model1}
\end{figure}

In the two tables to follow are shown the results obtained as regards the evaluation of the \textit{local correction} of our 
model on the typos dataset and the correction of the entire sequence with the \textit{Viterbi algorithm}.

\begin{figure}[H]
	\centering
	\begin{tabular}{lcc|cc}
		\toprule
		&\multicolumn{2}{c|}{\textsc{Edit Distance 1}} & \multicolumn{2}{c}{\textsc{Edit Distance 2}}\\
		& \texttt{big\_train}  & \texttt{big\_test} & \texttt{big\_train}  & \texttt{big\_test} \\
		\midrule
		\textbf{Num. observation} & \num{63759} & \num{15918} & \num{63759} & \num{15918} \\
		\textbf{Time (sec)}  & \num{53} & \num{14} & \num{1648} & \num{408} \\
		\textbf{Accuracy Top1} & \num{35,01}\%  & \num{35,60}\%  & \num{36,09}\%  & \num{36,84}\%  \\
		\textbf{Accuracy Top3} &  \num{46,24}\%  & \num{46,62}\%  & \num{48,61}\%  & \num{48,99}\%  \\
		\textbf{Accuracy Top5} & \num{50,19}\%  & \num{50,56}\%  & \num{52,86}\%  & \num{53,16}\%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Typos performance evaluation}
	\label{tab:typo-eval1}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 1}} \\
		\textbf{Dataset Perturbation} & \num{10}\%& \num{20}\% & \num{30}\%\\
		\midrule
		\textbf{Time (sec)}							 &\num{201}			&\num{199}			& \num{191}		\\
		\textbf{Perturbed correct} 			   & \num{64,06}\% &\num{60,16}\%  & \num{57,30}\% \\
		\textbf{Unperturbed not correct} &\num{41,70}\%	 &\num{43,36}\%  & \num{44,84}\%  \\
		\textbf{Exact match} 					  &\num{2,60}\%	   &\num{2,32}\%	&\num{1,56}\% \\
		\textbf{Accuracy} 							&\num{58,82}\%  &\num{57,47}\% &\num{55,73}\% \\
		\textbf{Precision}							 &\num{16,00}\% &\num{25,12}\% &\num{30,74}\% \\
		\textbf{Recall}									&\num{79,66}\% &\num{77,36}\%&\num{74,30}\%	\\
		\textbf{F-Measure}						  &\num{46,28}\% &\num{48,49}\%&\num{49,65}\%	\\
		\bottomrule
		%\vspace*{0.5em}
	\end{tabular}
		\begin{center}
		...
		\end{center}
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 2}} \\
		\textbf{Dataset Perturbation} & \num{10}\%& \num{20}\% & \num{30}\%  \\
		\midrule
		\textbf{Time (sec)}							 &\num{1110}		&\num{1121}	 	& \num{1116}\\
		\textbf{Perturbed correct} 			   & \num{54,96}\% &\num{55,47}\%  & \num{54,34}\%	 \\
		\textbf{Unperturbed not correct} &\num{55,19}\%	 &\num{56,31}\%  & \num{57,52}\%  \\
		\textbf{Exact match} 					  &\num{0,90}\%	   &\num{1,00}\%	&\num{0,76}\%\\
		\textbf{Accuracy} 							&\num{45,81}\%  &\num{46,00}\% &\num{45,85}\% \\
		\textbf{Precision}							 &\num{10,67}\% &\num{18,45}\% &\num{23,7}\% \\
		\textbf{Recall}									&\num{89,91}\% &\num{90,94}\%&\num{90,69}\%\\
		\textbf{F-Measure}						  &\num{38,77}\% &\num{41,55}\%&\num{44,37}\%	\\
		\bottomrule
	\end{tabular}

	\captionof{table}{Sentences performance evaluation}
	\label{tab:sentence-eval1}
\end{figure}

%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 1}} \\
%		\textbf{Dataset Perturbation} & \num{10}\%& \num{20}\% & \num{30}\%& \num{40}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{201}			&\num{199}			& \num{191}			&\num{186} \\
%		\textbf{Perturbed correct} 			   & \num{64,06}\% &\num{60,16}\%  & \num{57,30}\%	& \num{52,81}\% \\
%		\textbf{Unperturbed not correct} &\num{41,70}\%	 &\num{43,36}\%  & \num{44,84}\% & \num{46,58}\% \\
%		\textbf{Exact match} 					  &\num{2,60}\%	   &\num{2,32}\%	&\num{1,56}\%	&\num{1,42}\% \\
%		\textbf{Accuracy} 							&\num{58,82}\%  &\num{57,47}\% &\num{55,73}\% &\num{42,60}\% \\
%		\textbf{Precision}							 &\num{16,00}\% &\num{25,12}\% &\num{30,74}\%	&\num{33,75}\% \\
%		\textbf{Recall}									&\num{79,66}\% &\num{77,36}\%&\num{74,30}\%	&\num{70,40}\%\\
%		\textbf{F-Measure}						  &\num{46,28}\% &\num{48,49}\%&\num{49,65}\%	&\num{49,82}\%\\
%		\bottomrule
%		%\vspace*{0.5em}
%	\end{tabular}
%	\begin{center}
%		...
%	\end{center}
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 2}} \\
%		\textbf{Dataset Perturbation} & \num{10}\%& \num{20}\% & \num{30}\%& \num{40}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{1110}		&\num{1121}	 	& \num{1116}		&\num{1121} \\
%		\textbf{Perturbed correct} 			   & \num{54,96}\% &\num{55,47}\%  & \num{54,34}\%	& \num{51,76}\% \\
%		\textbf{Unperturbed not correct} &\num{55,19}\%	 &\num{56,31}\%  & \num{57,52}\% & \num{58,43}\% \\
%		\textbf{Exact match} 					  &\num{0,90}\%	   &\num{1,00}\%	&\num{0,76}\%	&\num{0,80}\% \\
%		\textbf{Accuracy} 							&\num{45,81}\%  &\num{46,00}\% &\num{45,85}\% &\num{45,14}\% \\
%		\textbf{Precision}							 &\num{10,67}\% &\num{18,45}\% &\num{23,7}\%	&\num{27,22}\% \\
%		\textbf{Recall}									&\num{89,91}\% &\num{90,94}\%&\num{90,69}\%	&\num{88,21}\%\\
%		\textbf{F-Measure}						  &\num{38,77}\% &\num{41,55}\%&\num{44,37}\%	&\num{46,18}\%\\
%		\bottomrule
%	\end{tabular}
%	
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval1}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment 2}
The second experiment was carried out using the same model described in the experiment above, with the 
difference that candidates are generated with lemmatisation.  

Language models can vary widely in content, depending on different factors such as datasets, preprocessing, and 
so on. An ideal scenario would be having every different conjugation, inflection and mood of a word in a language 
model, to assess its frequency in a certain language. But this is not always the case, and as our experimentation 
progressed, we found time and time again that a certain word, independently from its rarity, would be missing 
from our language model, and as such finding its inherent probability, $P(\mbox{word})$, would either be solved 
by defaulting to a base probability, or finding another solution.

The different solution came in the form of lemmatisation.
Lemmatisation takes into consideration the morphological analysis of the words. To do so, it is necessary to have 
detailed dictionaries which the algorithm can look through to link the form back to its lemma. Lemmatisation is 
more powerful than stemming because it doesn't consist only of a stemming algorithm, but also a dictionary to 
lookup the correct originating lemma.

We use one of the most popular packages for Natural Language Processing in Python, the \textbf{Natural 
Language Toolkit} (NLTK). We use the \textbf{WordNet} lexical database for lemmatisation.

Introducing lemmatisation gave us an improvement of about $2\%$ overall accuracy on the models, as, for example, 
\texttt{run} and \texttt{running} would both end up looking for $P(\mbox{run})$, and words that did not have a 
corresponding term in the language model most probably had the originating lemma in it. But we also found that in some 
cases the lemmatisation algorithm used a somewhat heuristic approach, for example considering both \texttt{books} and 
\texttt{bookses} to be valid plural forms of the \texttt{book} lemma. This meant that we could not assume that a word 
was correct if it resulted in a correct lemma contained in the language model, defeating our intended purpose for 
lemmatisation. Moreover the resulting models would lose the nuance among different words derived from a single lemma, 
which may have a different frequency of use.

The biggest issue we found was a large time overhead introduced by lemmatisation in the process of generating 
candidates, making the process last from an original \num{0,03} seconds to more than \num{0,33} seconds per word, which 
we deemed unreasonable for the real-time usage we had in mind for the project, thus abandoning the idea of 
lemmatisation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment 3}

The third experiment too was carried out on two different HMMs with the same edit distances as the previous 
experiments. 
The parameters on which the structure is based are shown in the table below:

\begin{figure}[H]
	\centering
	\begin{tabular}{cccc}
		\toprule
		max states 	& language model	&  sentence ds  &  train typo ds \\ \midrule
		\num{5} & \texttt{lotr\_language\_model} & \texttt{lotr\_clean}  & \texttt{big\_train} \\
		\bottomrule
	\end{tabular}
	\captionof{table}{HMM model}
	\label{tab:error_model3}
\end{figure}

In the two tables to follow are shown the results obtained as regards the evaluation of the \textit{local correction} 
of our model on the typos datasets \texttt{big\_test} and \texttt{lotr\_test} and the correction of the entire 
sequence with the \textit{Viterbi algorithm}.

\begin{figure}[H]
	\centering
	\begin{tabular}{lcc|cc}
		\toprule
		&\multicolumn{2}{c|}{\textsc{Edit Distance 1}} & \multicolumn{2}{c}{\textsc{Edit Distance 2}}\\
		& \texttt{big\_test} & \texttt{lotr\_test} & \texttt{big\_test} & \texttt{lotr\_test} \\
		\midrule
		\textbf{Num. observation} & \num{49959} & \num{12570} & \num{49959} & \num{12570} \\
		\textbf{Time (sec)}  		& \num{32} 				& \num{9} 			& \num{1207} 	& \num{309} \\
		\textbf{Accuracy Top1} & \num{39,75}\%  & \num{40,47}\%  & \num{63,71}\%  & \num{63,87}\%  \\
		\textbf{Accuracy Top3} &  \num{44,74}\%  & \num{45,34}\%  & \num{76,44}\%  & \num{76,75}\%  \\
		\textbf{Accuracy Top5} & \num{46,25}\%  & \num{46,92}\%  & \num{80,86}\%  & \num{80,80}\%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Typos performance evaluation}
	\label{tab:typo-eval3}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 1}} \\
		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%  \\
		\midrule
		\textbf{Time (sec)}							 &\num{116}			&\num{114}			& \num{114}			 \\
		\textbf{Perturbed correct} 			   & \num{79,97}\% &\num{76,55}\%  & \num{73,74}\%	 \\
		\textbf{Unperturbed not correct} & \num{13,68}\% &\num{14,46}\%  & \num{14,86}\%	\\
		\textbf{Exact match} 					  & \num{30,89}\% &\num{27,51}\%  & \num{25,23}\% \\
		\textbf{Accuracy} 							&\num{85,99}\%  &\num{84,62}\% &\num{83,32}\%  \\
		\textbf{Precision}							 & \num{28,99}\% &\num{41,97}\%  & \num{48,90}\%	\\
		\textbf{Recall}								  & \num{91,19}\% &\num{88,30}\%  & \num{86,09}\%	\\
		\textbf{F-Measure}						  & \num{74,19}\% &\num{72,87}\%  & \num{72,44}\% \\
		\bottomrule
		%\vspace*{0.5em}
	\end{tabular}
	\begin{center}
		...
	\end{center}
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 2}} \\
		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\% \\
		\midrule
		\textbf{Time (sec)}							 &\num{866}			&\num{869}			& \num{854}	 \\
		\textbf{Perturbed correct} 			   & \num{73,26}\% &\num{72,05}\%  & \num{71,10}\% \\
		\textbf{Unperturbed not correct} & \num{18,47}\% &\num{18,86}\%  & \num{19,46}\%	 \\
		\textbf{Exact match} 					  & \num{22,74}\% &\num{21,26}\%  & \num{19,52}\%	 \\
		\textbf{Accuracy} 							&\num{81,07}\%  &\num{80,17}\% &\num{79,06}\% \\
		\textbf{Precision}							 & \num{22,40}\% &\num{34,49}\%  & \num{41,35}\%		\\
		\textbf{Recall}								  & \num{94,31}\% &\num{94,16}\%  & \num{93,63}\%	 \\
		\textbf{F-Measure}						  & \num{68,76}\% &\num{68,60}\%  & \num{68,53}\% \\
		\bottomrule
	\end{tabular}
	
	\captionof{table}{Sentences performance evaluation}
	\label{tab:sentence-eval3}
\end{figure}


%
%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 1}} \\
%		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%& \num{20}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{116}			&\num{114}			& \num{114}			&\num{112} \\
%		\textbf{Perturbed correct} 			   & \num{79,97}\% &\num{76,55}\%  & \num{73,74}\%	& \num{71,76}\% \\
%		\textbf{Unperturbed not correct} & \num{13,68}\% &\num{14,46}\%  & \num{14,86}\%	& \num{15,69}\% \\
%		\textbf{Exact match} 					  & \num{30,89}\% &\num{27,51}\%  & \num{25,23}\%	& \num{22,66}\% 
%\\
%		\textbf{Accuracy} 							&\num{85,99}\%  &\num{84,62}\% &\num{83,32}\% &\num{81,73}\% \\
%		\textbf{Precision}							 & \num{28,99}\% &\num{41,97}\%  & \num{48,90}\%	& \num{52,85}\% 
%\\
%		\textbf{Recall}								  & \num{91,19}\% &\num{88,30}\%  & \num{86,09}\%	& \num{84,31}\% \\
%		\textbf{F-Measure}						  & \num{74,19}\% &\num{72,87}\%  & \num{72,44}\%	& \num{71,97}\% \\
%		\bottomrule
%		%\vspace*{0.5em}
%	\end{tabular}
%	\begin{center}
%		...
%	\end{center}
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 2}} \\
%		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%& \num{20}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{866}			&\num{869}			& \num{854}			&\num{849} \\
%		\textbf{Perturbed correct} 			   & \num{73,26}\% &\num{72,05}\%  & \num{71,10}\%	& \num{69,90}\% \\
%		\textbf{Unperturbed not correct} & \num{18,47}\% &\num{18,86}\%  & \num{19,46}\%	& \num{20,16}\% \\
%		\textbf{Exact match} 					  & \num{22,74}\% &\num{21,26}\%  & \num{19,52}\%	& \num{18,38}\% 
%\\
%		\textbf{Accuracy} 							&\num{81,07}\%  &\num{80,17}\% &\num{79,06}\% &\num{77,79}\% \\
%		\textbf{Precision}							 & \num{22,40}\% &\num{34,49}\%  & \num{41,35}\%	& \num{45,37}\% 
%\\
%		\textbf{Recall}								  & \num{94,31}\% &\num{94,16}\%  & \num{93,63}\%	& \num{93,43}\% \\
%		\textbf{F-Measure}						  & \num{68,76}\% &\num{68,60}\%  & \num{68,53}\%	& \num{68,58}\% \\
%		\bottomrule
%	\end{tabular}
%	
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval3}
%\end{figure}


%\textbf{FIXME}
%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lcccc}
%		\toprule
%		& Time (sec)  & Accuracy Top1 & Accuracy Top3  &  Accuracy Top5 \\
%		\midrule
%		Train & \num{20986} & \num{41,38}\%  & \num{57,28} \% & \num{61,60} \% \\
%		Test &	\num{5270}  & \num{56,78}\%  & \num{74,75} \% & \num{80,59} \%  \\
%		\bottomrule
%	\end{tabular}
%	\captionof{table}{Typos performance evaluation}
%	\label{tab:typo-eval3}
%\end{figure}

% con 10% ho il 17% di errore
% con 10% ho il 25% di errore
% con 20% ho il 40% 31 di errore

%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lccc|ccc}
%		\toprule
%		\textbf{Edit Distance} & \multicolumn{3}{c|}{1} & \multicolumn{3}{c}{2}\\
%		\textbf{Dataset Perturbation} & \num{10}\% & \num{15}\%& \num{20}\% & \num{10}\% & \num{15}\%& 
%		\num{20}\% \\
%		\midrule
%		Time (sec) &\num{165}&\num{157}& \num{147}&\num{2920}&\num{2835}&\num{2869}\\
%		\midrule
%		Perturbed correct & \num{81,33}\% &\num{76,48}\%& \num{71,58}\%& \num{61,33}\% 
%		&\num{60,99}\% 
%		&\num{59,91}\%\\
%		Unperturbed not correct &\num{41,62}\%&\num{42,34}\% & \num{43,94}\% & \num{61,75}\% & 
%		\num{61,61}\% & 
%		\num{62,09}\%\\
%		Exact match &\num{5,49}\%&\num{5,35}\%&\num{5,06}\%&\num{1,76}\%&\num{2,06}\%&\num{2,06}\%\\
%		Accuracy &\num{60,84}\% &\num{61,49}\% &\num{60,59}\% &\num{40,70}\% &\num{43,08}\% 
%		&\num{44,39}\% \\
%		Precision&\num{21,94}\%&\num{33,61}\% &\num{39,7}\%&\num{12,41}\%&\num{21,22}\%&\num{27,06}\%\\
%		Recall&\num{94,59}\%&\num{90,05}\%&\num{85,97}\%&\num{99,41}\%&\num{98,58}\%&\num{97,71}\%\\
%		F-Measure&\num{50,55}\%&\num{54,58}\%&\num{57,09}\%&\num{39,16}\%&\num{43,67}\%&\num{47,36}\%\\
%		\bottomrule
%	\end{tabular}
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval3a}
%\end{figure}
%
%
%\begin{figure}[H]
%	\centering
%	\begin{tabular}{ccccccc}
%		\toprule
%		\#sentence & Time (sec)  & Accuracy & Initial Error  &  Precision & Recall & Specificity \\
%		\midrule
%		\num{1620}	& \num{2705}  & \num{58,21}\%  & \num{17,39}\% & \num{90,30}\% & \num{58,62}\%  & 
%		\num{8,43}\%  
%		\\
%		\bottomrule
%	\end{tabular}
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval3}
%\end{figure}