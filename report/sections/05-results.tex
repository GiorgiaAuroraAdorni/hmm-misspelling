\chapter{Experiment and Results}

\section{Evaluation Metrics}
We evaluate the performance of the model on individual typos through various measures of accuracy. In particular, we 
compute the \textsc{Top-1 Accuracy}, comparing the intended word and the best candidate predicted by the 
model. Then we compute the \textsc{Top-3 Accuracy} and \textsc{Top-5 Accuracy}, comparing the intended word 
with the first \num{3} and \num{5} candidates produced by the model, respectively.\\

To evaluate the performance on entire sentences, we save a CSV file containing the sentences with our perturbations, 
the sentences that we want to predict (hidden truth) and the sentences produced by the model. We then iterate through 
the lines of this file and, for each word we verify if it was perturbed or not and if it corresponds to the original 
truth. Therefore, the following cases can occur for each word:
\begin{enumerate}
	\item \textit{Perturbed word, not correctly predicted}
	\begin{enumerate}
		\item the model did not attempt to correct the word
		\item the model attempted to correct the word, but without success
	\end{enumerate}
	\item \textit{Perturbed word, correctly predicted}
	\item \textit{Unperturbed word, not correctly predicted}
	\item \textit{Unperturbed word, correctly predicted}
\end{enumerate}

From these four cases we can construct two confusion matrices that represent, respectively, the ability of the model to 
detect errors and the ability to provide the right corrections, depending on the classification of Cases 1(a). and 1(b).


\begin{figure}[H]
	\centering
	\begin{tabular}{lc|cc}
		\toprule
		& & \multicolumn{2}{c}{\textbf{Model Prediction}}\\
		& & \textsc{Detected}  & \textsc{Not Detected} \\
		\midrule
		\multirow{4}{*}{\shortstack[c]{\textbf{Hidden}\\ \textbf{Truth}}} 
		& \multirow{2}{*}{\textsc{Perturbed}}   & True Positive & False Negative	\\ 
		& &  Case 1(b). \& Case 2. & Case 1(a).	\\ 
		& \multirow{2}{*}{\textsc{Unperturbed}}  & False Positive & True Negative	\\
		& &  Case 3.  & Case 4.	\\ 
		\bottomrule
	\end{tabular}
	\captionof{table}{Confusion matrix - Error Detection}
	\label{tab:confmat-detection}
\end{figure}

From the confusion matrix defined in the table above, it is possible to calculate the following standard performance 
metrics:
\begin{itemize}
	\item \textsc{Detection-Accuracy}: percentage of words where the model correctly detects whether it was perturbed or 
	not
	\[ \frac{\mbox{True Positive} + \mbox{True Negative}}{\sum \mbox{All}} = \frac{\mbox{Case 1(b).} + 
	\mbox{Case 2.} + \mbox{Case 4.}}{\mbox{Case 1.} + \mbox{Case 2.} + \mbox{Case 3.} + \mbox{Case 4.}}\]
	\item \textsc{Detection-Recall}: proportion of the detected errors among all the errors
	\[\frac{\mbox{True Positive} }{\mbox{True Positive} + \mbox{False Negative} } = \frac{\mbox{Case 1(b).} + 
	\mbox{Case 2.}}{\mbox{Case 1.} + \mbox{Case 2.}}\]
	\item \textsc{Detection-Precision}: ratio of the correct detections with respect to all detections
		\[ \frac{\mbox{True Positive}}{\mbox{True Positive} + \mbox{False Positive}} = \frac{\mbox{Case 1(b). + 
		\mbox{Case 2.}}}{\mbox{Case 1(b).}+  \mbox{Case 2.} + \mbox{Case 3.}}\]
\end{itemize}

\begin{figure}[H]
	\centering
	\begin{tabular}{lc|ccc}
		\toprule
		& & \multicolumn{3}{c}{\textbf{Model Prediction}}\\
		& & \multirow{2}{*}{\textsc{Corrected}}  &  \textsc{Wrogly} & \multirow{2}{*}{\textsc{Not Corrected}}  \\
		& &  &\textsc{Corrected}  &  \\ 
		\midrule
		\multirow{2}{*}{\shortstack[c]{\textbf{Hidden}\\ \textbf{Truth}}} 
		& \textsc{Perturbed}  & Case 2. & Case 1(b). & Case 1(a).	\\ 
		& \textsc{Unperturbed}  & -  & Case 3.  & Case 4.	\\ 
		\bottomrule
	\end{tabular}
	\captionof{table}{Confusion matrix - Error Correction}
	\label{tab:confma-error}
\end{figure}

From the second confusion matrix defined in the Table \ref{tab:confma-error}, it is possible to calculate the following 
metrics:
\begin{itemize}
	\item \textsc{Correction-Accuracy}: percentage of words where the model prediction matches the 
	ground truth  
	\[ \frac{\mbox{Case 2.} + \mbox{Case 4.}}{\mbox{Case 1(a).}+ \mbox{Case 1(b).}+ \mbox{Case 2.} + \mbox{Case 3.} 
	+ \mbox{Case 4.}}\]
	\item \textsc{Correction-Recall}: ratio of perturbed words correctly predicted
	\[ \frac{\mbox{Case 2.}}{\mbox{Case 2.} + \mbox{Case 1(a).}+ \mbox{Case 1(b).}}\]
	\item \textsc{Correction-Precision}: ratio of appropriate corrections with respect to all corrections proposed by the 
	model
		\[ \frac{\mbox{Case	2.}}{\mbox{Case 1(b).}+ \mbox{Case 2.} + \mbox{Case 3.}}\]
\end{itemize}

The \textsc{Specificity}, that is the ratio of unperturbed words correctly predicted, computed in the same way from both 
the confusion matrices:
\[ \frac{\mbox{True Negative}}{\sum \mbox{Unperturbed}} = \frac{\mbox{Case 4.}}{\mbox{Case 3.} + 
	\mbox{Case 4.}}\]

\section{Experiments}
We performed three different types of experiments.

The first one using as a transition model the dataset \texttt{big\_clean}, the associate perturbed dataset and test error 
models, and the language model \texttt{frequency-alpha-gcide}.

A second one using the same datasets but introducing a lemmatisation consisting in a simple dictionary lookup.

The last one using as transition model the dataset \texttt{lotr\_clean}, the associate perturbed dataset and test error 
models, and the language model \texttt{lotr\_language\_model}.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment 1}
The first experiment was carried out on two different HMM that differ in the choice of the \texttt{max\_edits} 
parameter.
In the first case, in fact, the edit distance considered was $1$, while in the second $2$.
In both cases the HMM is structured as follows:

\begin{figure}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
				max states 	& language model	&  sentence ds  &  train typo ds 	&  test typo ds\\ \midrule
				\num{5} & \texttt{frequency-alpha-gcide} & \texttt{big\_clean}  & \texttt{big\_train}  &\texttt{big\_test}\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Experiment 1 - HMM model parameters}
	\label{tab:error_model1}
\end{figure}

In the two tables to follow are shown the results obtained as regards the evaluation of the \textit{local correction} of our 
model on the typos dataset and the correction of the entire sequence with the \textit{Viterbi algorithm}.

\begin{figure}[H]
	\centering
	\begin{tabular}{lcc|cc}
		\toprule
		&\multicolumn{2}{c|}{\textsc{Edit Distance 1}} & \multicolumn{2}{c}{\textsc{Edit Distance 2}}\\
		& \texttt{big\_train}  & \texttt{big\_test} & \texttt{big\_train}  & \texttt{big\_test} \\
		\midrule
		\textbf{Num. observation} & \num{63759} & \num{15918} & \num{63759} 	& \num{15918} \\
		\textbf{Time}  		& \num{54}s 			& \num{14}s		& \num{1660}s			& \num{419}s \\
		\textbf{Accuracy Top1} & \num{34,96}\%  & \num{35,46}\%  & \num{37,34}\%  & \num{37,95}\%  \\
		\textbf{Accuracy Top3} &  \num{46,05}\%  & \num{46,41}\%  & \num{49,01}\%  & \num{49,25}\%  \\
		\textbf{Accuracy Top5} & \num{50,18}\%  & \num{50,52}\%  & \num{53,40}\%  & \num{53,65}\%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Experiment 1 - Typos performance evaluation}
	\label{tab:typo-eval1}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 1}} \\
		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%\\
		\midrule
		\textbf{Time}							 &\num{40}s			&\num{38}s			& \num{39}s		\\
		\textbf{Exact Sentence Match} 		 &\num{27,37}\%	   &\num{20,98}\%	&\num{13,19}\% \\
		\textbf{Detection Accuracy} 		 &\num{90,39}\%  &\num{89,04}\% &\num{86,79}\% \\
		\textbf{Detection Recall}				&\num{82,35}\% &\num{80,94}\%&\num{77,42}\%	\\
		\textbf{Detection Precision}			&\num{55,15}\% &\num{69,22}\%&\num{76,67}\%	\\
		\textbf{Correction Accuracy} 		&\num{88,75}\%  &\num{85,96}\% &\num{81,66}\% \\
		\textbf{Correction Recall}				&\num{67,17}\% &\num{65,71}\%&\num{58,67}\%	\\
		\textbf{Correction Precision}			&\num{44,18}\% &\num{55,74}\%&\num{58,07}\%	\\
		\textbf{Specificity} 							&\num{91,48}\%  &\num{90,99}\% &\num{90,56}\% \\
		\bottomrule
		%\vspace*{0.5em}
	\end{tabular}
		\begin{center}
		...
		\end{center}
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 2}} \\
		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%  \\
		\midrule
		\textbf{Time}							 		&\num{223}s			&\num{225}s			& \num{226}s		\\
		\textbf{Exact Sentence Match} 		&\num{24,98}\%	   &\num{20,88}\%	&\num{16,48}\% \\
		\textbf{Detection Accuracy} 		 &\num{89,58}\%  &\num{89,61}\% &\num{89,33}\% \\
		\textbf{Detection Recall}				&\num{92,63}\% &\num{92,37}\%&\num{90,35}\%	\\
		\textbf{Detection Precision}			&\num{53,47}\% &\num{68,58}\%&\num{77,01}\%	\\
		\textbf{Correction Accuracy} 		&\num{86,99}\%  &\num{84,94}\% &\num{82,48}\% \\
		\textbf{Correction Recall}				&\num{68,74}\% &\num{68,97}\%&\num{65,29}\%	\\
		\textbf{Correction Precision}			&\num{38,49}\% &\num{50,35}\%&\num{55,60}\%	\\
		\textbf{Specificity} 							&\num{89,29}\%  &\num{89,08}\% &\num{89,07}\% \\
		\bottomrule
	\end{tabular}

	\captionof{table}{Experiment 1 - Sentences performance evaluation}
	\label{tab:sentence-eval1}
\end{figure}

%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 1}} \\
%		\textbf{Dataset Perturbation} & \num{10}\%& \num{20}\% & \num{30}\%& \num{40}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{201}			&\num{199}			& \num{191}			&\num{186} \\
%		\textbf{Perturbed correct} 			   & \num{64,06}\% &\num{60,16}\%  & \num{57,30}\%	& \num{52,81}\% \\
%		\textbf{Unperturbed not correct} &\num{41,70}\%	 &\num{43,36}\%  & \num{44,84}\% & \num{46,58}\% \\
%		\textbf{Exact match} 					  &\num{2,60}\%	   &\num{2,32}\%	&\num{1,56}\%	&\num{1,42}\% \\
%		\textbf{Accuracy} 							&\num{58,82}\%  &\num{57,47}\% &\num{55,73}\% &\num{42,60}\% \\
%		\textbf{Precision}							 &\num{16,00}\% &\num{25,12}\% &\num{30,74}\%	&\num{33,75}\% \\
%		\textbf{Recall}									&\num{79,66}\% &\num{77,36}\%&\num{74,30}\%	&\num{70,40}\%\\
%		\textbf{F-Measure}						  &\num{46,28}\% &\num{48,49}\%&\num{49,65}\%	&\num{49,82}\%\\
%		\bottomrule
%		%\vspace*{0.5em}
%	\end{tabular}
%	\begin{center}
%		...
%	\end{center}
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 2}} \\
%		\textbf{Dataset Perturbation} & \num{10}\%& \num{20}\% & \num{30}\%& \num{40}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{1110}		&\num{1121}	 	& \num{1116}		&\num{1121} \\
%		\textbf{Perturbed correct} 			   & \num{54,96}\% &\num{55,47}\%  & \num{54,34}\%	& \num{51,76}\% \\
%		\textbf{Unperturbed not correct} &\num{55,19}\%	 &\num{56,31}\%  & \num{57,52}\% & \num{58,43}\% \\
%		\textbf{Exact match} 					  &\num{0,90}\%	   &\num{1,00}\%	&\num{0,76}\%	&\num{0,80}\% \\
%		\textbf{Accuracy} 							&\num{45,81}\%  &\num{46,00}\% &\num{45,85}\% &\num{45,14}\% \\
%		\textbf{Precision}							 &\num{10,67}\% &\num{18,45}\% &\num{23,7}\%	&\num{27,22}\% \\
%		\textbf{Recall}									&\num{89,91}\% &\num{90,94}\%&\num{90,69}\%	&\num{88,21}\%\\
%		\textbf{F-Measure}						  &\num{38,77}\% &\num{41,55}\%&\num{44,37}\%	&\num{46,18}\%\\
%		\bottomrule
%	\end{tabular}
%	
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval1}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment 2}
The second experiment was carried out using the same model described in the experiment above, with the 
difference that candidates are generated with lemmatisation.  

Language models can vary widely in content, depending on different factors such as datasets, preprocessing, and 
so on. An ideal scenario would be having every different conjugation, inflection and mood of a word in a language 
model, to assess its frequency in a certain language. But this is not always the case, and as our experimentation 
progressed, we found time and time again that a certain word, independently from its rarity, would be missing 
from our language model, and as such finding its inherent probability, $P(\mbox{word})$, would either be solved 
by defaulting to a base probability, or finding another solution.

The different solution came in the form of lemmatisation.
Lemmatisation takes into consideration the morphological analysis of the words. To do so, it is necessary to have 
detailed dictionaries which the algorithm can look through to link the form back to its lemma. Lemmatisation is 
more powerful than stemming because it doesn't consist only of a stemming algorithm, but also a dictionary to 
lookup the correct originating lemma.

We use one of the most popular packages for Natural Language Processing in Python, the \textbf{Natural 
Language Toolkit} (NLTK), with the \textbf{WordNet} lexical database for lemmatisation.

Introducing lemmatisation gave us an improvement of about $2\%$ overall accuracy on the models, as, for example, 
\texttt{run} and \texttt{running} would both end up looking for $P(\mbox{run})$, and words that did not have a 
corresponding term in the language model most probably had the originating lemma in it. But we also found that in some 
cases the lemmatisation algorithm used a somewhat heuristic approach, for example considering both \texttt{books} and 
\texttt{bookses} to be valid plural forms of the \texttt{book} lemma. This meant that we could not assume that a word 
was correct if it resulted in a correct lemma contained in the language model, defeating our intended purpose for 
lemmatisation. Moreover the resulting models would lose the nuance among different words derived from a single lemma, 
which may have a different frequency of use.

The biggest issue we found was a large time overhead introduced by lemmatisation in the process of generating 
candidates, making the process last from an original \num{0,03} seconds to more than \num{0,33} seconds per word, which 
we deemed unreasonable for the real-time usage we had in mind for the project, thus abandoning the idea of 
lemmatisation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment 3}

The third experiment too was carried out on two different HMMs with the same edit distances as the previous 
experiments. 
The parameters on which the structure is based are shown in the table below:

\begin{figure}[H]
	\centering
	\begin{tabular}{cccc}
		\toprule
		max states 	& language model	&  sentence ds  &  train typo ds \\ \midrule
		\num{5} & \texttt{lotr\_language\_model} & \texttt{lotr\_clean}  & \texttt{big\_train} \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Experiment 3 - HMM model parameters}
	\label{tab:error_model3}
\end{figure}

This experiment tries to investigate the performance of a model tailored to a specific writing style. In particular 
we train the initial probabilities and transition probabilities from the “Lord of the Rings” books. The two tables that 
follow show the results obtained from the evaluation of the \textit{local correction} of our model on the typos 
datasets \texttt{big\_test} and \texttt{lotr\_test} and the correction of the entire sequence with the \textit{Viterbi 
algorithm}.

As expected, the performance on the \texttt{lotr\_test} dataset are significantly higher than in Experiment 1, since it 
uses the same language of the text corpus the model was trained on. Somewhat surprisingly, the performance hit on 
\texttt{big\_test} was larger than expected, with accuracies as much as 15\% worse. This shows the importance of 
correctly choosing the training datasets based on the target writing style.

\begin{figure}[H]
	\centering
	\begin{tabular}{lcc|cc}
		\toprule
		&\multicolumn{2}{c|}{\textsc{Edit Distance 1}} & \multicolumn{2}{c}{\textsc{Edit Distance 2}}\\
		& \texttt{big\_test} & \texttt{lotr\_test} & \texttt{big\_test} & \texttt{lotr\_test} \\
		\midrule
		\textbf{Num. observation} & \num{15918} & \num{12570} & \num{15918} & \num{12570} \\
		\textbf{Time}  					& \num{13}s			& \num{10}s 		& \num{374}s 	& \num{312}s \\
		\textbf{Accuracy Top1} & \num{28,28}\%  & \num{40,37}\%  & \num{28,06}\%  & \num{62,26}\%  \\
		\textbf{Accuracy Top3} &  \num{39,00}\%  & \num{45,27}\%  & \num{37,98}\%  & \num{75,90}\%  \\
		\textbf{Accuracy Top5} & \num{45,14}\%  & \num{46,92}\%  & \num{42,15}\%  & \num{80,39}\%  \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Experiment 3 - Typos performance evaluation}
	\label{tab:typo-eval3}
\end{figure}



\begin{figure}[H]
	\centering
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 1}} \\
		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%\\
		\midrule
		\textbf{Time}							 &\num{23}s			&\num{23}s			& \num{23}s		\\
		\textbf{Exact Sentence Match} 		 &\num{78,82}\%	   &\num{67,63}\%	&\num{59,04}\% \\
		\textbf{Detection Accuracy} 		 &\num{99,30}\%  &\num{98,42}\% &\num{97,68}\% \\
		\textbf{Detection Recall}				&\num{90,57}\% &\num{87,00}\%&\num{87,36}\%	\\
		\textbf{Detection Precision}			&\num{97,17}\% &\num{98,27}\%&\num{98,48}\%	\\
		\textbf{Correction Accuracy} 		&\num{98,90}\%  &\num{97,23}\% &\num{97,68}\% \\
		\textbf{Correction Recall}				&\num{82,09}\% &\num{76,21}\%&\num{75,81}\%	\\
		\textbf{Correction Precision}			&\num{89,05}\% &\num{86,44}\%&\num{85,30}\%	\\
		\textbf{Specificity} 							&\num{99,86}\%  &\num{99,83}\% &\num{99,69}\% \\
		\bottomrule
		%\vspace*{0.5em}
	\end{tabular}
	\begin{center}
		...
	\end{center}
	\begin{tabular}{lccc}
		\toprule
		&\multicolumn{3}{c}{\textsc{Edit Distance 2}} \\
		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%  \\
		\midrule
		\textbf{Time}							 		&\num{173}s			&\num{175}s			& \num{174}s		\\
		\textbf{Exact Sentence Match} 		&\num{79,42}\%	   &\num{69,53}\%	&\num{63,94}\% \\
		\textbf{Detection Accuracy} 		 &\num{99,57}\%  &\num{99,24}\% &\num{98,81}\% \\
		\textbf{Detection Recall}				&\num{94,09}\% &\num{93,25}\%&\num{93,35}\%	\\
		\textbf{Detection Precision}			&\num{99,23}\% &\num{99,79}\%&\num{99,61}\%	\\
		\textbf{Correction Accuracy} 		&\num{98,95}\%  &\num{97,60}\% &\num{96,75}\% \\
		\textbf{Correction Recall}				&\num{81,51}\% &\num{78,75}\%&\num{79,97}\%	\\
		\textbf{Correction Precision}			&\num{86,79}\% &\num{84,15}\%&\num{85,62}\%	\\
		\textbf{Specificity} 							&\num{99,95}\%  &\num{99,96}\% &\num{99,93}\% \\
		\bottomrule
	\end{tabular}
	\captionof{table}{Experiment 3 - Sentences performance evaluation}
\label{tab:sentence-eval3}
\end{figure}

%
%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 1}} \\
%		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%& \num{20}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{116}			&\num{114}			& \num{114}			&\num{112} \\
%		\textbf{Perturbed correct} 			   & \num{79,97}\% &\num{76,55}\%  & \num{73,74}\%	& \num{71,76}\% \\
%		\textbf{Unperturbed not correct} & \num{13,68}\% &\num{14,46}\%  & \num{14,86}\%	& \num{15,69}\% \\
%		\textbf{Exact match} 					  & \num{30,89}\% &\num{27,51}\%  & \num{25,23}\%	& \num{22,66}\% 
%\\
%		\textbf{Accuracy} 							&\num{85,99}\%  &\num{84,62}\% &\num{83,32}\% &\num{81,73}\% \\
%		\textbf{Precision}							 & \num{28,99}\% &\num{41,97}\%  & \num{48,90}\%	& \num{52,85}\% 
%\\
%		\textbf{Recall}								  & \num{91,19}\% &\num{88,30}\%  & \num{86,09}\%	& \num{84,31}\% \\
%		\textbf{F-Measure}						  & \num{74,19}\% &\num{72,87}\%  & \num{72,44}\%	& \num{71,97}\% \\
%		\bottomrule
%		%\vspace*{0.5em}
%	\end{tabular}
%	\begin{center}
%		...
%	\end{center}
%	\begin{tabular}{lcccc}
%		\toprule
%		&\multicolumn{4}{c}{\textsc{Edit Distance 2}} \\
%		\textbf{Dataset Perturbation} & \num{5}\%& \num{10}\% & \num{15}\%& \num{20}\%  \\
%		\midrule
%		\textbf{Time (sec)}							 &\num{866}			&\num{869}			& \num{854}			&\num{849} \\
%		\textbf{Perturbed correct} 			   & \num{73,26}\% &\num{72,05}\%  & \num{71,10}\%	& \num{69,90}\% \\
%		\textbf{Unperturbed not correct} & \num{18,47}\% &\num{18,86}\%  & \num{19,46}\%	& \num{20,16}\% \\
%		\textbf{Exact match} 					  & \num{22,74}\% &\num{21,26}\%  & \num{19,52}\%	& \num{18,38}\% 
%\\
%		\textbf{Accuracy} 							&\num{81,07}\%  &\num{80,17}\% &\num{79,06}\% &\num{77,79}\% \\
%		\textbf{Precision}							 & \num{22,40}\% &\num{34,49}\%  & \num{41,35}\%	& \num{45,37}\% 
%\\
%		\textbf{Recall}								  & \num{94,31}\% &\num{94,16}\%  & \num{93,63}\%	& \num{93,43}\% \\
%		\textbf{F-Measure}						  & \num{68,76}\% &\num{68,60}\%  & \num{68,53}\%	& \num{68,58}\% \\
%		\bottomrule
%	\end{tabular}
%	
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval3}
%\end{figure}


%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lcccc}
%		\toprule
%		& Time (sec)  & Accuracy Top1 & Accuracy Top3  &  Accuracy Top5 \\
%		\midrule
%		Train & \num{20986} & \num{41,38}\%  & \num{57,28} \% & \num{61,60} \% \\
%		Test &	\num{5270}  & \num{56,78}\%  & \num{74,75} \% & \num{80,59} \%  \\
%		\bottomrule
%	\end{tabular}
%	\captionof{table}{Typos performance evaluation}
%	\label{tab:typo-eval3}
%\end{figure}

% con 10% ho il 17% di errore
% con 10% ho il 25% di errore
% con 20% ho il 40% 31 di errore

%\begin{figure}[H]
%	\centering
%	\begin{tabular}{lccc|ccc}
%		\toprule
%		\textbf{Edit Distance} & \multicolumn{3}{c|}{1} & \multicolumn{3}{c}{2}\\
%		\textbf{Dataset Perturbation} & \num{10}\% & \num{15}\%& \num{20}\% & \num{10}\% & \num{15}\%& 
%		\num{20}\% \\
%		\midrule
%		Time (sec) &\num{165}&\num{157}& \num{147}&\num{2920}&\num{2835}&\num{2869}\\
%		\midrule
%		Perturbed correct & \num{81,33}\% &\num{76,48}\%& \num{71,58}\%& \num{61,33}\% 
%		&\num{60,99}\% 
%		&\num{59,91}\%\\
%		Unperturbed not correct &\num{41,62}\%&\num{42,34}\% & \num{43,94}\% & \num{61,75}\% & 
%		\num{61,61}\% & 
%		\num{62,09}\%\\
%		Exact match &\num{5,49}\%&\num{5,35}\%&\num{5,06}\%&\num{1,76}\%&\num{2,06}\%&\num{2,06}\%\\
%		Accuracy &\num{60,84}\% &\num{61,49}\% &\num{60,59}\% &\num{40,70}\% &\num{43,08}\% 
%		&\num{44,39}\% \\
%		Precision&\num{21,94}\%&\num{33,61}\% &\num{39,7}\%&\num{12,41}\%&\num{21,22}\%&\num{27,06}\%\\
%		Recall&\num{94,59}\%&\num{90,05}\%&\num{85,97}\%&\num{99,41}\%&\num{98,58}\%&\num{97,71}\%\\
%		F-Measure&\num{50,55}\%&\num{54,58}\%&\num{57,09}\%&\num{39,16}\%&\num{43,67}\%&\num{47,36}\%\\
%		\bottomrule
%	\end{tabular}
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval3a}
%\end{figure}
%
%
%\begin{figure}[H]
%	\centering
%	\begin{tabular}{ccccccc}
%		\toprule
%		\#sentence & Time (sec)  & Accuracy & Initial Error  &  Precision & Recall & Specificity \\
%		\midrule
%		\num{1620}	& \num{2705}  & \num{58,21}\%  & \num{17,39}\% & \num{90,30}\% & \num{58,62}\%  & 
%		\num{8,43}\%  
%		\\
%		\bottomrule
%	\end{tabular}
%	\captionof{table}{Sentences performance evaluation}
%	\label{tab:sentence-eval3}
%\end{figure}